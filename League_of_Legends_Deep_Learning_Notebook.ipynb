{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea5cc4b-f3ae-4ee0-a026-65da9b8c7f63",
   "metadata": {},
   "source": [
    "# League of Legends Performance Predictor\n",
    "Zachary Schwartz\n",
    "4/28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832be0be-7270-4ebb-bb89-f1eca7e7a4cb",
   "metadata": {},
   "source": [
    "# Background\n",
    "League of Legends is a 5v5 battle, where both teams are attempting to destroy the opponents base. Each player selects one of 168 champions to play, and a specific role (also known as position) to play in the game. Most champions are only playable in one or two roles, and similarly most people that play the game are only good at one or two roles. People tend to play the roles and champions they are best at, which will make up most of the data used in this project. In the game, there are objectives that players will fight over, and destroying them is beneficial to them team. Getting more objectives than the opposing team is generally a sign of a winning team. As well, when I refer to statistics, I am referring to a list of performance metrics that can give a sense of how well a player performed, which I will get more into later.  \n",
    "  \n",
    "There is a ranked ladder that is made up of different divisions. Since I would like to use the model myself, I gathered data that was similarish to my skill level. I gathered games from these ranks, in order from highest to lowest skill: Challenger, Grandmaster, Masters, Diamond, and Emerald. Challenger is the top 300 players in a region, and grandmaster is the top 1000. Masters has roughly 4000 people in it (Best 0.5% of players). Challenger, Grandmaster, Masters will sometimes be referred to as Masters+. Diamond and Emerald are both split into 4 divisions from most to least skilled: I, II, III, and IV.  \n",
    "  \n",
    "Personally, I am ranked usually on the lower end of masters, and I played competitevly for IWU all of my years here. This year we ranked as a top 64 team in the country, and I wanted to do a project that could somehow augment my play, or just generally be based on the game that I love  \n",
    "  \n",
    "Note: The data gathering aspect of the code will not work since the developer api key is needed to run it, which resets every 24 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be42436-1da9-40c5-94a4-ac14596e9e88",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this project is to use deep learning to predict what players are going to do in a league of legends game. More specifically, each player can be represented by multiple statistics that can be accessed after a game. These stats can be seen in the **player_columns** list below, but involve damage dealt, damage taken, assists, etc. The model is End to End, meaning that it involves one network that processes all of the data. There are 1760 inputs into the model, and 256 outputs. The inputs are made up from the players in a game, and the champions they are playing. Each player or champion is represented by 24 statistics, each statistic being represented by 4 values, the 25th, 50th, 75 percentile, and the standard deviation. 4 values per statistic, multiplied by 22 statistics, multiplied by 10 players and 10 champions, giving us 1760 inputs. The outputs are represnted by 24 statistics per player, however there is only 1 value per statistic, and only a value given for the 10 players in the game. That leaves us with 240 values, and then an extra 13 statistics (1 for each team) to represent the objectives present in the game, leaving us off with 266 predictions. These are towers and other neutral objectives that provide value for a team when destroyed, and the model will predict how many of each are destroyed by which team, the length of the game, and ultimately who ends up winning as well.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae1b9a-0c6c-42c9-9929-5d0cb6fa8e41",
   "metadata": {},
   "source": [
    "If this model turns out to be effective, it could be a very useful tool for competitive games. The model could be run with your opponents stats, and could show your team what champions they are strong with, which would provide a massive advantage in the drafting phase when champions are picked and banned. Conversely, it could also give your own team a better sense of what your best picks are, and what synergies work and don't work.  \n",
    "  \n",
    "However, my hypothesis going into this project is that it would be unable to make effective predictions. There are many limitations to what could affect the overall accuracy of the model. Are people playing worse than usual? Can player impact not be accurately detected by their statistics? Is the data I've gathered now outdated due to recent balance changes? Is there not enough data? Does the IQR + STD provide enough info for a model? Did I make the right choices in which statistics to involve or remove? Was an End to End model a good choice, or should I have had separate networks to analyze players/champions separately?  \n",
    "  \n",
    "After doing some research, it does appear that match outcome has been predicted by deep learning models, however it doesnt seem like they tried to predict the actual stats of the players in a game like my project is attempting. I can use this paper as a benchmark to see if my methods were effective in at least figuring out which team ended up winning with the same accuracy that this paper has.\n",
    "https://dl.acm.org/doi/abs/10.1145/3472538.3472579#:~:text=In%20this%20paper%2C%20we%20propose,which%20occurs%20before%20gameplay%20begins.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a1045-559f-42ec-a5dc-dfa8a818077b",
   "metadata": {},
   "source": [
    "Below is setup, importing various modules I will use, and setting up global variables to be used throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6d6205-5f74-471a-8a88-85f14032d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import requests\n",
    "import time\n",
    "import sqlite3\n",
    "import json\n",
    "import threading\n",
    "import torch\n",
    "import torch.nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "region_list = [\n",
    "    \"br1\",\n",
    "    \"eun1\",\n",
    "    \"euw1\",\n",
    "    \"jp1\",\n",
    "    \"kr\",\n",
    "    \"la1\",\n",
    "    \"la2\",\n",
    "    \"na1\",\n",
    "    \"oc1\",\n",
    "    \"ph2\",\n",
    "    \"ru\",\n",
    "    \"sg2\",\n",
    "    \"th2\",\n",
    "    \"tr1\",\n",
    "    \"tw2\",\n",
    "    \"vn2\",\n",
    "]\n",
    "\n",
    "continents_dictionary = {\n",
    "    \"americas\": [\"na1\", \"br1\", \"la1\", \"la2\"],\n",
    "    \"asia\": [\"jp1\", \"kr\"],\n",
    "    \"europe\": [\"eun1\", \"euw1\", \"ru\", \"tr1\"],\n",
    "    \"sea\": [\"oc1\", \"ph2\", \"sg2\", \"th2\", \"tw2\", \"vn2\"],\n",
    "}\n",
    "\n",
    "league_list = [\"challenger\", \"grandmaster\", \"master\"]\n",
    "division_list = [\"I\", \"II\", \"III\", \"IV\"]\n",
    "roles = [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]\n",
    "match_file_path = \"match_database.db\"\n",
    "api_key = \"\"\n",
    "\n",
    "player_columns =   [\n",
    "    \"assists\",\n",
    "    \"champLevel\",\n",
    "    \"damageDealtToObjectives\",\n",
    "    \"damageSelfMitigated\",\n",
    "    \"deaths\",\n",
    "    \"goldSpent\",\n",
    "    \"killingSprees\",\n",
    "    \"kills\",\n",
    "    \"largestKillingSpree\",\n",
    "    \"largestMultiKill\",\n",
    "    \"timeCCingOthers\",\n",
    "    \"totalDamageDealt\",\n",
    "    \"totalDamageDealtToChampions\",\n",
    "    \"totalDamageShieldedOnTeammates\",\n",
    "    \"totalDamageTaken\",\n",
    "    \"totalHeal\",\n",
    "    \"totalHealsOnTeammates\",\n",
    "    \"totalMinionsKilled\",\n",
    "    \"totalTimeSpentDead\",\n",
    "    \"visionWardsBoughtInGame\",\n",
    "    \"wardsKilled\",\n",
    "    \"wardsPlaced\",\n",
    "    \"gameDuration\",\n",
    "    \"win\"\n",
    "]\n",
    "\n",
    "team_columns = [\n",
    "    \"win\",\n",
    "    \"BaronFirst\",\n",
    "    \"ChampionFirst\",\n",
    "    \"DragonFirst\",\n",
    "    \"InhibitorFirst\",\n",
    "    \"RiftHeraldFirst\",\n",
    "    \"TurretFirst\",\n",
    "    \"BaronKills\",\n",
    "    \"ChampionKills\",\n",
    "    \"DragonKills\",\n",
    "    \"InhibitorKills\",\n",
    "    \"RiftHeraldKills\",\n",
    "    \"TurretKills\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a055f4-0a31-4db6-b6c1-f1e7654c77d2",
   "metadata": {},
   "source": [
    "As it is important to set up global variables, it is also important to create the databases as well. It takes up a lot of space but here is the code that would create them if running this notebook from scratch. This is also valuable to read through to know what statistics I am using as predictions, and as labels. The columns in champ_stats and player_stats are the inputs for the model, and the columns in player_matches and team_matches are the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ca6fff-20b7-4b42-9bc1-2e7181be763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conn = sqlite3.connect(match_file_path)\n",
    "match_cursor = match_conn.cursor()\n",
    "match_cursor.execute('''CREATE TABLE IF NOT EXISTS players (\n",
    "                    summonerid TEXT,\n",
    "                    rank TEXT,\n",
    "                    region TEXT,\n",
    "                    puuid TEXT\n",
    "                  )''')\n",
    "match_conn.close()\n",
    "\n",
    "\n",
    "\n",
    "match_cursor.execute('''CREATE TABLE IF NOT EXISTS player_matches (\n",
    "                    matchId INTEGER,\n",
    "                    puuid TEXT,\n",
    "                    teamPosition TEXT,\n",
    "                    championName TEXT,\n",
    "                    gameDuration INTEGER,\n",
    "                    win INTEGER,\n",
    "                    assists INTEGER,\n",
    "                    champLevel INTEGER,\n",
    "                    damageDealtToObjectives INTEGER,\n",
    "                    damageSelfMitigated INTEGER,\n",
    "                    deaths INTEGER,\n",
    "                    goldSpent INTEGER,\n",
    "                    killingSprees INTEGER,\n",
    "                    kills INTEGER,\n",
    "                    largestKillingSpree INTEGER,\n",
    "                    largestMultiKill INTEGER,\n",
    "                    timeCCingOthers INTEGER,\n",
    "                    totalDamageDealt INTEGER,\n",
    "                    totalDamageDealtToChampions INTEGER,\n",
    "                    totalDamageShieldedOnTeammates INTEGER,\n",
    "                    totalDamageTaken INTEGER,\n",
    "                    totalHeal INTEGER,\n",
    "                    totalHealsOnTeammates INTEGER,\n",
    "                    totalMinionsKilled INTEGER,\n",
    "                    totalTimeSpentDead INTEGER,\n",
    "                    visionWardsBoughtInGame INTEGER,\n",
    "                    wardsKilled INTEGER,\n",
    "                    wardsPlaced INTEGER\n",
    "                  )''')\n",
    "match_conn.commit()\n",
    "\n",
    "match_cursor.execute('''CREATE TABLE IF NOT EXISTS champ_stats (\n",
    "                    championName TEXT,\n",
    "                    teamPosition TEXT,\n",
    "                    assists INTEGER,\n",
    "                    champLevel INTEGER,\n",
    "                    damageDealtToObjectives INTEGER,\n",
    "                    damageSelfMitigated INTEGER,\n",
    "                    deaths INTEGER,\n",
    "                    goldSpent INTEGER,\n",
    "                    killingSprees INTEGER,\n",
    "                    kills INTEGER,\n",
    "                    largestKillingSpree INTEGER,\n",
    "                    largestMultiKill INTEGER,\n",
    "                    timeCCingOthers INTEGER,\n",
    "                    totalDamageDealt INTEGER,\n",
    "                    totalDamageDealtToChampions INTEGER,\n",
    "                    totalDamageShieldedOnTeammates INTEGER,\n",
    "                    totalDamageTaken INTEGER,\n",
    "                    totalHeal INTEGER,\n",
    "                    totalHealsOnTeammates INTEGER,\n",
    "                    totalMinionsKilled INTEGER,\n",
    "                    totalTimeSpentDead INTEGER,\n",
    "                    visionWardsBoughtInGame INTEGER,\n",
    "                    wardsKilled INTEGER,\n",
    "                    wardsPlaced INTEGER\n",
    "                  )''')\n",
    "match_conn.commit()\n",
    "match_cursor.execute('''CREATE TABLE IF NOT EXISTS player_stats (\n",
    "                    puuid TEXT,\n",
    "                    teamPosition TEXT,\n",
    "                    assists INTEGER,\n",
    "                    champLevel INTEGER,\n",
    "                    damageDealtToObjectives INTEGER,\n",
    "                    damageSelfMitigated INTEGER,\n",
    "                    deaths INTEGER,\n",
    "                    goldSpent INTEGER,\n",
    "                    killingSprees INTEGER,\n",
    "                    kills INTEGER,\n",
    "                    largestKillingSpree INTEGER,\n",
    "                    largestMultiKill INTEGER,\n",
    "                    timeCCingOthers INTEGER,\n",
    "                    totalDamageDealt INTEGER,\n",
    "                    totalDamageDealtToChampions INTEGER,\n",
    "                    totalDamageShieldedOnTeammates INTEGER,\n",
    "                    totalDamageTaken INTEGER,\n",
    "                    totalHeal INTEGER,\n",
    "                    totalHealsOnTeammates INTEGER,\n",
    "                    totalMinionsKilled INTEGER,\n",
    "                    totalTimeSpentDead INTEGER,\n",
    "                    visionWardsBoughtInGame INTEGER,\n",
    "                    wardsKilled INTEGER,\n",
    "                    wardsPlaced INTEGER\n",
    "                  )''')\n",
    "match_conn.commit()\n",
    "\n",
    "match_cursor.execute('''CREATE TABLE IF NOT EXISTS match_stats (\n",
    "                    matchId TEXT,\n",
    "                    win INTEGER,\n",
    "                    BaronFirst INTEGER,\n",
    "                    ChampionFirst INTEGER,\n",
    "                    DragonFirst INTEGER,\n",
    "                    InhibitorFirst INTEGER,\n",
    "                    RiftHeraldFirst INTEGER,\n",
    "                    TurretFirst INTEGER,\n",
    "                    BaronKills INTEGER,\n",
    "                    ChampionKills INTEGER,\n",
    "                    DragonKills INTEGER,\n",
    "                    InhibitorKills INTEGER,\n",
    "                    RiftHeraldKills INTEGER,\n",
    "                    TurretKills INTEGER\n",
    "                  )''')\n",
    "match_conn.commit()\n",
    "\n",
    "match_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ba475-149d-47a7-ac7d-b3127081c68c",
   "metadata": {},
   "source": [
    "# Part 1: Data Gathering\n",
    "An equally important step to the actual training of the model for this project was in creating the dataset the model was to be trained on. The basic steps of which can be seen below  \n",
    "1. Setup a riot developer account, which allows me to query their databases for the information I will use https://developer.riotgames.com/\n",
    "2. Every account has a summonerId and a puuid (Player Universally Unique IDentifiers). The summonerIds can be queried many at a time by the different ranks, and will be our querying the databse for information. https://developer.riotgames.com/apis#league-v4/GET_getLeagueEntries\n",
    "3. Unfortunately although summonerIds are easier to obtain than the puuids, I can only access a players match history through their puuid, so the next step will be finding the puuids. I can then query riots database with a summoner Id, and get the associated puuids. https://developer.riotgames.com/apis#summoner-v4/GET_getBySummonerId\n",
    "4. Now that I have every players puuid, the next step is gathering a list of every match played. Each match has a unique id, and I can query the database for a list of matchIds associated with a player https://developer.riotgames.com/apis#match-v5/GET_getMatchIdsByPUUID\n",
    "5. Now I can now query with a matchId, and be returned all of the statistics associated. https://developer.riotgames.com/apis#match-v5/GET_getMatch\n",
    "6. For the last bit of organizing, I now need to set up our inputs to the model. Since every player and champion will be represented by one large tensor of values, each of which that are calculated from finding averages and standard deviations, it would take the model a very long time to run those calculations each time to run the model. Becaue of this, I can run these calculations once for each player and champion, and then store the result since looking up precomputed data is much quicker than recomputing it.  \n",
    "   \n",
    "Put simply, gather summonerIds, use summonerIds to gather puuids, use puuids to gather matchId list, use matchId list to gather match statistics, use match statistics to generate calculations for the model input  \n",
    "  \n",
    "Other things of note: There is a rate limit of 200 calls per minute, per region or continent depending on the kind of call. If you as the reader would get your own developer key, you would see some of these functions run significantly faster than others because of whether they can query a region or continent. Continents are just made up of multiple regions, for example the Americas are made up of the north american, brazilian, and two separate latin american servers. Running all of these would take over 24 hours in one sitting.  \n",
    "  \n",
    "**Databases**  \n",
    "  \n",
    "The tables I made throughout this project went through many modifications (generally getting smaller as I learned what data I didnt actually need.) I originally created two databases, which although was not ultimately necessary, it did reduce the size of both databases, and made them faster to download and upload individually. My thinking to create the two databases was that I wanted the statistics related to matches in a separate place from the secondary info (user ids and list of matchids) that was needed to acquire them in the first place. The first database was the player database, which stored the summonerId, rank, region, puuid, and eventually the list of matchIds played in a region. Steps 2-4 involve the player database.  \n",
    "  \n",
    "The match database has the bulk of the information stored, with 4 separate tables. Firstly, the player_matches holds all the statistics and other identifying information for each game played. Each matchId has 10 separate rows, one for each player and their performances. This table was used directly to create the rest the inputs and outputs. The first 4 columns are identifying info, matchid, puuid, champion, and role. The next 2 columns are used for labels, and the last 24 columns are the statistics recorded for the player in that game. The next two tables data are calculations based on the information found in player_matches  \n",
    "  \n",
    "player_stats and champ_stats both hold averages based on the statistics gathered. These two tables make up the inputs to the model. In champ_stats each row is a champion, and the roles they have played in the data. (Most champions have about 4 rows, but ususally about 2 of those have less than 10 games since the champion isn't meant to be played there). The rest of the columns are dedicated to the stastistics I gathered that are being used to make the predictions. The actual values stored is a list of 4 values, the 25th, 50th, 75th percentile, and the standard deviation, for each column. player_stats is the exact same as champ_stats, but instead of champion names they are puuids instead. When gathering inputs to the model, a given match id will find out what champions and players participated in the game, and will then find each of those precalculated statistics from these tables.\n",
    "  \n",
    "team_matches holds the objective information for both the teams in a match, thus there are two rows per matchId. It records which team won, the first team to take each objective, and how many of the objectives they gathered. This data is only used for labels in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09026263-cea3-49a3-b472-87dbc38ac5f7",
   "metadata": {},
   "source": [
    "All of the data gathering functions were built with their own call function, that allows each to be called one by one in case the api key resets.  \n",
    "  \n",
    "Generally, the functions mentioned in steps 2-5 above all follow the same structure. They accept a region, and possibly ranks as well as inputs. Then in a while loop will request values from the api based upon the arguments given, and the purpose of the function. A while loop is used rather than a for loop since it can handle errors more easily in case of database or server errors. If the request is successful (a 200 status code is recieved), then the value is loaded and inserted into a database, and then a counter variable is incremented to move onto the next request. There are also try excepts to help debug and see where issues arise. The error code of 429 means that the function must wait a few seconds before requesting again. I believe this was generally the most effective style of retrieving and storing data.  \n",
    "  \n",
    "for get_all_players specifically, its made to handle both masters+ and diamond/emerald ranks despite them having different api requests, to minimize duplicated code. The call_get_all_players function has a few for loops to make sure to find all the players in their differing regions and ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e55c08c-2ab1-4402-bf9b-bf27b5a0da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_players(region, rank, division):\n",
    "    # Gathers every summonerId in masters +, diamond, and emerald\n",
    "    player_conn = sqlite3.connect(match_file_path)\n",
    "    player_cursor = player_conn.cursor() # Create connection to database\n",
    "    count = 1\n",
    "    while True:\n",
    "        # Handles which request to send depending on if the league is masters+ or not\n",
    "        if division != \"\":\n",
    "            request = requests.get(f\"https://{region}.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/{rank.upper()}/{division}?page={str(count)}&api_key={api_key}\")\n",
    "        else:\n",
    "            request = requests.get(f\"https://{region}.api.riotgames.com/lol/league/v4/{rank}leagues/by-queue/RANKED_SOLO_5x5?page={str(count)}&api_key={api_key}\")\n",
    "    \n",
    "        if request.status_code == 200: # If the request is successful, input basic info about the user\n",
    "            \n",
    "            try:\n",
    "                users = json.loads(request.text)\n",
    "                if users == []:\n",
    "                    break\n",
    "                    \n",
    "                for record in users:\n",
    "                    # Input the data used to find more about these players\n",
    "                    player_cursor.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO players (summonerId, rank, region)\n",
    "                        VALUES (?, ?, ?)\n",
    "                        \"\"\",\n",
    "                        (\n",
    "                            record[\"summonerId\"],\n",
    "                            rank + division,\n",
    "                            region,\n",
    "                        ),\n",
    "                    )\n",
    "                    player_conn.commit()\n",
    "                    count += 1\n",
    "                    \n",
    "            except Exception as e: # Except statement in case of error\n",
    "                print(\"Error processing API response:\", e)\n",
    "                \n",
    "        elif request.status_code == 429: # Should I get rate limited, the program will wait to try to request info again\n",
    "            print(region, division)\n",
    "            time.sleep(10)\n",
    "            \n",
    "        else:\n",
    "            print(request.status_code)\n",
    "\n",
    "def call_get_all_players():\n",
    "    # Stores every summonerid from every region from emerald IV to challenger in player database\n",
    "    threads = []\n",
    "    for region in region_list: # creates a unique thread for each region, then calls the puuid finder\n",
    "        # First loops through masters+ leagues, then loops through the different divisions of diamond and emerald\n",
    "        for league in league_list:\n",
    "            thread = threading.Thread(target=get_all_players, args=(region, league, \"\",)) # Using threading makes this function run significantly faster\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            print(\"new_thread created\")\n",
    "    \n",
    "        for division in division_list:\n",
    "            thread = threading.Thread(target=get_all_players, args=(region, \"diamond\", division,)) # Using threading makes this function run significantly faster\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "    \n",
    "            thread = threading.Thread(target=get_all_players, args=(region, \"emerald\", division,)) # Using threading makes this function run significantly faster\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "    for thread in threads: # ensures the main thread waits for all threads to finish\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99552169-e46f-46f0-8efa-421c5e07eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_puuids_by_region(region):\n",
    "    # Given a region, places every users puuid into the database\n",
    "    region_conn = sqlite3.connect(match_file_path)\n",
    "    region_cursor = region_conn.cursor()\n",
    "    \n",
    "    summoner_id_list = region_cursor.execute(\"SELECT summonerId FROM players WHERE region = ?\", (region,)).fetchall() # create list of every summonerid, then loop through the list\n",
    "    summoner_ids = [row[0] for row in summoner_id_list]\n",
    "    index = 0\n",
    "\n",
    "    # Makes request to server for the information regarding this summonerid\n",
    "    while index < len(summoner_ids):\n",
    "        request = requests.get(\n",
    "            \"https://\"\n",
    "            + region\n",
    "            + \".api.riotgames.com/lol/summoner/v4/summoners/\"\n",
    "            + summoner_ids[index]\n",
    "            + api_key\n",
    "        )\n",
    "        \n",
    "        if request.status_code == 200: # If successful, find the matching puuid for the given summoner id\n",
    "            try:\n",
    "                user = json.loads(request.text)\n",
    "                region_cursor.execute(\n",
    "                    \"\"\"\n",
    "                            UPDATE players\n",
    "                            SET puuid = ?\n",
    "                            WHERE summonerId = ?\n",
    "                        \"\"\",\n",
    "                    (user[\"puuid\"], summoner_ids[index]),\n",
    "                )\n",
    "                region_conn.commit()\n",
    "                index += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"Error processing API response:\", e)\n",
    "                \n",
    "        elif request.status_code == 429:\n",
    "            # If too many requests were sent, wait a few moments before trying again\n",
    "            print(\"sleeping\")\n",
    "            time.sleep(20)\n",
    "            \n",
    "        else:\n",
    "            print(request.status_code)\n",
    "\n",
    "\n",
    "def call_puuids():\n",
    "    threads = []\n",
    "    for region in region_list: # Creates a unique thread for each region, then calls the puuid finder\n",
    "        thread = threading.Thread(target=get_puuids_by_region, args=(region,)) # Using threading makes this function run significantly faster\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        print(\"new_thread created\")\n",
    "\n",
    "    for thread in threads: # ensures the main thread waits for all threads to finish\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826856fd-9b16-480b-b482-718e6503a32e",
   "metadata": {},
   "source": [
    "Now that all of the puuids are acquired, the next part is to start looking for all the matches played. This function had a unique issue, where I couldn't just accept every single matchId played because then I would have 10 repeats of the same match in our data. That would pose an issue in the next function where I would then gather data based on every matchId, and if there are repeats I could end up with up to 120 rows of data about the same game, when I should only have 12 (10 for each of the players, 2 for each team). To solve this, I added every matchId found to one set. The nature of sets is that they will not accept non unique values, which meant I could add every matchId found to it, without having to worry about conditional statements. The downside of this is that one large set isn't really ideal for database management, since now all the matchIds found in a region would go into just one row. There may be a better way to handle that, however my function didn't pose me any major issues. Otherwise this function functions similarly to the others, except that it only has one insert statement at the end, as opposed to throughout the while loop like the others. The while loop in this function merely adds the found matchIds to the set. This is also the first function that now works with continents, which slows down the speed at which this function runs compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a98b4f-419c-454e-bf85-f681ebb8d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_played(continent, regions):\n",
    "    # Given a continent and regions, collect every unique matchId played for each region\n",
    "    continent_conn = sqlite3.connect(match_file_path)\n",
    "    continent_cursor = continent_conn.cursor()\n",
    "\n",
    "    for region in regions: # loop through all regions within the given continent\n",
    "        puuid_list = continent_cursor.execute(\n",
    "            \"SELECT puuid FROM players WHERE region = ?\", (region,)\n",
    "        ).fetchall()\n",
    "        puuids = [row[0] for row in puuid_list] # find all puuids for the given region\n",
    "        index = 0 # tracks where in the puuids the function is\n",
    "        match_count = 0 # This variable tracks where to begin the search in a players history\n",
    "        repeat_matches = set() # Holds the list of mathes for a region, since it is a set it will not take non unique values\n",
    "\n",
    "        # In case the databse already has some matches, this code will add them to repeat_matches so they dont get mixed up\n",
    "        # repeat_list = continent_cursor.execute(\n",
    "        #     \"SELECT matches FROM players WHERE region = ?\", (region,)\n",
    "        # ).fetchall()\n",
    "        # [repeat_matches.add(row[0]) for row in repeat_list]\n",
    "\n",
    "        # This code loops requests every match played by a user this season, and adds them to the set\n",
    "        # Loop increments by 100 to look at the next 100 matches, and only moves on from a user once the request comes back empty\n",
    "        while index < len(puuids):\n",
    "            request = requests.get(\n",
    "                f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuids[index]}/ids?startTime=16417728&type=ranked&start={str(match_count)}&count=100&{api_key}\"\n",
    "            )\n",
    "            if request.status_code == 200:\n",
    "                \n",
    "                try:\n",
    "                    matches = json.loads(request.text)\n",
    "                    \n",
    "                    if len(matches) == 0: # Base Case for incrementing the while loop, moving on from the user\n",
    "                        index += 1\n",
    "                        continue\n",
    "                        \n",
    "                    for match in matches:\n",
    "                        repeat_matches.add(match) # Adds the matches to the set\n",
    "                    match_count += 100 # The request brings back 100 matches at a time, so this code looks at next 100\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"Error processing API response:\", e)\n",
    "                    \n",
    "            elif request.status_code == 429:\n",
    "                # If too many requests were sent, wait a few moments before trying again\n",
    "                print(\"sleeping\" + continent + region + str(index))\n",
    "                time.sleep(20)\n",
    "                \n",
    "            else:\n",
    "                print(request.status_code)\n",
    "\n",
    "        # Perhaps suboptimal database management, but I place the entire list of matches in 1 row per region\n",
    "        # This occurs after the while loop has finished finding every match\n",
    "        continent_cursor.execute(\n",
    "                            \"\"\"\n",
    "                                UPDATE players\n",
    "                                SET matches = ?\n",
    "                                WHERE puuid = ?\n",
    "                            \"\"\",\n",
    "                            (json.dumps(list(repeat_matches)), puuids[1]),\n",
    "                                )\n",
    "        continent_conn.commit()\n",
    "\n",
    "\n",
    "def call_match_list():\n",
    "    # Calls function that stores every match played\n",
    "    threads = []\n",
    "    for continent, regions in continents_dictionary.items():\n",
    "        thread = threading.Thread(\n",
    "            target=get_matches_played,\n",
    "            args=(continent,regions,),)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        print(\"new_thread created\")\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a2a81-8fcf-4fc5-a508-333f5007c574",
   "metadata": {},
   "source": [
    "This function went through many iterations, since I wasn't sure which features I would end up using for the model. Because of the way I stored the matchIds previously, there is some string splicing required at the beginning to make the list iterable. Ultimately though it isn't much different from the previous functions, it requests the statistics and places some into player rows, and some into team rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09f3126-925c-4fc1-847f-521565d57a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_matches(continent, regions):\n",
    "    # Stores all values from a match into the match database, from every match\n",
    "    player_conn = sqlite3.connect(match_file_path)\n",
    "    player_cursor = player_conn.cursor()\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    \n",
    "    for region in regions:\n",
    "        # Gather list the list of all matches in a region\n",
    "        print(region)\n",
    "        match_list = player_cursor.execute(\n",
    "            \"SELECT matches FROM players WHERE region = ?\", (region,)\n",
    "        ).fetchall()\n",
    "        \n",
    "        matches = [row[0] for row in match_list] # the match ids are stored as a list with one element, a giant string\n",
    "        \n",
    "        # Some string splicing needs to be done in order to properly loop through the list\n",
    "        matches = matches[0][1:-1]\n",
    "        matches = [match[1:-2] for match in matches.split()] # this line splits this string into a list that can be looped through\n",
    "        \n",
    "        match_index = 0 # index for overall match list\n",
    "        \n",
    "        while match_index < len(matches):\n",
    "            request = requests.get(\n",
    "                f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/{matches[match_index]}?api_key={api_key}\"\n",
    "            )\n",
    "            if request.status_code == 200:\n",
    "                try:\n",
    "                    match_data = json.loads(request.text)\n",
    "\n",
    "                    # Create variables for ease of use later\n",
    "                    info = match_data[\"info\"]\n",
    "                    teams = info[\"teams\"]\n",
    "                    # Inserts the two team rows into database\n",
    "                    for team in teams:\n",
    "                        objectives = team['objectives']\n",
    "                        match_cursor.execute(\n",
    "                            \"\"\"\n",
    "                            INSERT INTO team_matches (\n",
    "                                matchId,\n",
    "                                win,\n",
    "                                BaronFirst,\n",
    "                                ChampionFirst,\n",
    "                                DragonFirst,\n",
    "                                InhibitorFirst,\n",
    "                                RiftHeraldFirst,\n",
    "                                TurretFirst,\n",
    "                                BaronKills,\n",
    "                                ChampionKills,\n",
    "                                DragonKills,\n",
    "                                InhibitorKills,\n",
    "                                RiftHeraldKills,\n",
    "                                TurretKills\n",
    "                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                            \"\"\",\n",
    "                            (matches[match_index],\n",
    "                            int(team['win']),\n",
    "                            objectives['baron']['first'],\n",
    "                            objectives['champion']['first'],\n",
    "                            objectives['dragon']['first'],\n",
    "                            objectives['inhibitor']['first'],\n",
    "                            objectives['riftHerald']['first'],\n",
    "                            objectives['tower']['first'],\n",
    "                            objectives['baron']['kills'],\n",
    "                            objectives['champion']['kills'],\n",
    "                            objectives['dragon']['kills'],\n",
    "                            objectives['inhibitor']['kills'],\n",
    "                            objectives['riftHerald']['kills'],\n",
    "                            objectives['tower']['kills'],\n",
    "                            )\n",
    "                        )\n",
    "                    # Inserts the 10 rows of participants into database\n",
    "                    participants = info[\"participants\"]\n",
    "                    for participant in participants:\n",
    "                        match_cursor.execute(\n",
    "                        \"\"\"\n",
    "                        INSERT INTO player_matches (\n",
    "                                    \"matchId\",\n",
    "                                    \"puuid\",\n",
    "                                    \"teamPosition\",\n",
    "                                    \"championName\",\n",
    "                                    \"gameDuration\",\n",
    "                                    \"win\",\n",
    "                                    \"assists\",\n",
    "                                    \"champLevel\",\n",
    "                                    \"damageDealtToObjectives\",\n",
    "                                    \"damageSelfMitigated\",\n",
    "                                    \"deaths\",\n",
    "                                    \"goldSpent\",\n",
    "                                    \"killingSprees\",\n",
    "                                    \"kills\",\n",
    "                                    \"largestKillingSpree\",\n",
    "                                    \"largestMultiKill\",\n",
    "                                    \"timeCCingOthers\",\n",
    "                                    \"totalDamageDealt\",\n",
    "                                    \"totalDamageDealtToChampions\",\n",
    "                                    \"totalDamageShieldedOnTeammates\",\n",
    "                                    \"totalDamageTaken\",\n",
    "                                    \"totalHeal\",\n",
    "                                    \"totalHealsOnTeammates\",\n",
    "                                    \"totalMinionsKilled\",\n",
    "                                    \"totalTimeSpentDead\",\n",
    "                                    \"visionWardsBoughtInGame\",\n",
    "                                    \"wardsKilled\",\n",
    "                                    \"wardsPlaced\"\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\",\n",
    "                        (\n",
    "                            matches[match_index],\n",
    "                            participant['puuid'],\n",
    "                            participant['teamPosition'],\n",
    "                            participant['championName'],\n",
    "                            info['gameDuration'],\n",
    "                            participant['win'],\n",
    "                            participant['assists'],\n",
    "                            participant['champLevel'],\n",
    "                            participant['damageDealtToObjectives'],\n",
    "                            participant['damageSelfMitigated'],\n",
    "                            participant['deaths'],\n",
    "                            participant['goldSpent'],\n",
    "                            participant['killingSprees'],\n",
    "                            participant['kills'],\n",
    "                            participant['largestKillingSpree'],\n",
    "                            participant['largestMultiKill'],\n",
    "                            participant['timeCCingOthers'],\n",
    "                            participant['totalDamageDealt'],\n",
    "                            participant['totalDamageDealtToChampions'],\n",
    "                            participant['totalDamageShieldedOnTeammates'],\n",
    "                            participant['totalDamageTaken'],\n",
    "                            participant['totalHeal'],\n",
    "                            participant['totalHealsOnTeammates'],\n",
    "                            participant['totalMinionsKilled'],\n",
    "                            participant['totalTimeSpentDead'],\n",
    "                            participant['visionWardsBoughtInGame'],\n",
    "                            participant['wardsKilled'],\n",
    "                            participant['wardsPlaced']\n",
    "                        ),\n",
    "                    )\n",
    "                \n",
    "                    match_conn.commit()\n",
    "                    match_index += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # End case\n",
    "                    print(\"Error processing API response:\", e)\n",
    "                    if \"list index out of range\" in str(e):\n",
    "                        match_index +=1\n",
    "                        continue\n",
    "                        \n",
    "            elif request.status_code == 429:\n",
    "                # If too many requests were sent, wait a few moments before trying again\n",
    "                print(\"sleeping \" + continent + \" \" + region + \" \" + str(match_index))\n",
    "                time.sleep(10)\n",
    "                \n",
    "            elif request.status_code == 404:\n",
    "                # If match doesnt have any info, continue on\n",
    "                match_index += 1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                print(request.status_code)\n",
    "\n",
    "\n",
    "def call_store_matches():\n",
    "    # Calls function that stores every statistic from every match played\n",
    "    threads = []\n",
    "    for continent, regions in continents_dictionary.items():\n",
    "        thread = threading.Thread(\n",
    "            target=store_matches,\n",
    "            args=(continent, regions, ),)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        print(\"new_thread created\")\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5306d-39bc-4b07-bcb4-bc0d59a898a4",
   "metadata": {},
   "source": [
    "To gather data on players, there is an issue of the amount of data played. Some people play one role and a few champs for hundreds of games, some people play a little of everything, and some barely play at all. One possible choice was using an RNN or GRU, which takes data of varying lengths, modifying its outputs as it takes in the random length of data. I decided against it since I didn't believe it would really fit the problem very well. Moreso, a large part of RNNs is deciding importance of inputs, and I imagine each game should generally be as equally valuable as the last. That being said there have been improvements that handle those issues, and an RNN based model could be an idea for a future project  \n",
    "  \n",
    "To capture information on a wild gaps in the amount of games played, I decided to combine it all into key statistics that capture the essence of the data. Essentially statistics that the model could understand the general idea for how well this player or champion performed as a certain statistic. Since only 4 values are stored per statistics, this also speeds up the model significantly. This function prepares this data in an sqlite database, taking in a champion or player, then the role they played. Every statistic on them is called that I've stored, and the percentiles are taken from those values.  \n",
    "  \n",
    "The main issue of this approach is the case where there is one game of someone playing a role once, or the champion playing in a role they aren't supposed to. The rest of the data in that game could be valuable, so I don't want to completely throw out or neglect games with offrole players or champions. This is most apparent because some of these games they may perform incredibly well or poorly and have wide swings, affecting data quality. Another side effect of this issue is when they play only 1 game, the torch.std doesn't work on a single value, and returns NaN isntead. Although I could have gone into the database to change the values, I elected to just handle the case in find_stats later on. That may increase the time it takes to create a dataset object because of it. Ideally I would have so much data, that any time there is an offrole player or champion I could ignore that game, but I want to use every piece of data I can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a802a7cc-943c-410c-8cde-23046a9f7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_stats(column_type, name, role, table):\n",
    "    # Given a champion name or puuid, enter a tensor of the iqr and std into champ_stats\n",
    "    # This is not used in the actual model, but was instead created to have the values ready, rather than having to run the calculations during training\n",
    "    \n",
    "    stats = torch.tensor([])\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    # Gather all the games of a player or champion in that specific role\n",
    "    # The stats for a playe or character will be different based on the role played, so its important not to combine them.\n",
    "    column_vals = match_cursor.execute(f\"SELECT * FROM player_matches WHERE {column_type} = ? AND teamPosition = ?\", (name, str(role))).fetchall()\n",
    "\n",
    "    # Loop through each of the player statistic columns, ignoring the last 2 since they are part of what I am are predicting\n",
    "    match_cursor.execute(f\"INSERT INTO {table} ({column_type}, teamPosition) VALUES (?, ?)\", (name, role))\n",
    "    match_conn.commit()\n",
    "    for index, column in enumerate(player_columns[:-2]):\n",
    "        data = torch.tensor([])\n",
    "        # Loop through every row to gather each instance of the statistic\n",
    "        for row in column_vals:\n",
    "            data = torch.cat((data, torch.tensor([float(row[index+6])])))\n",
    "        \n",
    "        # Gather the iqr of the statistics\n",
    "        iqr = torch.quantile(data, torch.tensor([0.25, 0.5, 0.75])).unsqueeze(0)\n",
    "        stats = torch.cat((stats, iqr), dim=0)\n",
    "        \n",
    "        # Gather the standard deviation\n",
    "        std_dev = torch.std(data).unsqueeze(0).unsqueeze(0)\n",
    "        stats = torch.cat((stats, std_dev), dim=1)\n",
    "        # This line was supposed to fix the NaN issue, but it didn't work\n",
    "        torch.nan_to_num(stats)\n",
    "        \n",
    "        # Convert into a format that sqlite accepts\n",
    "        stats_list = stats.numpy()\n",
    "        stats_list = json.dumps(stats_list.tolist())\n",
    "        match_cursor.execute(f\"UPDATE {table} SET {column} = ? WHERE {column_type} = '{name}' AND teamPosition = '{role}'\", (stats_list,))\n",
    "        match_conn.commit()\n",
    "        \n",
    "        # Reset the variables\n",
    "        stats = torch.empty(0)\n",
    "    match_conn.close()\n",
    "\n",
    "\n",
    "def call_gather_stats():\n",
    "    # Creates player_stats and champ_stats which will be called in the training of the model\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    \n",
    "    # Gather every champion and user\n",
    "    champion_names = match_cursor.execute(\"SELECT DISTINCT championName FROM player_matches\").fetchall()\n",
    "    player_names = match_cursor.execute(\"SELECT DISTINCT puuid FROM player_matches\").fetchall()\n",
    "    match_conn.close()\n",
    "\n",
    "    # Loop through every champion, user and their roles to create the tables\n",
    "    for champ in champion_names:\n",
    "        for role in roles:\n",
    "            gather_stats(\"championName\", champ[0], role, \"champ_stats\")\n",
    "    for player in player_names:\n",
    "        for role in roles:\n",
    "            gather_stats(\"puuid\", player[0], role, \"player_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a571d2c-33cb-4ba5-a9d4-0d93145ebed5",
   "metadata": {},
   "source": [
    "To simplify the calls, this function calls all of the separate calls to the data gathering process in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91ad563-d305-4062-99b5-9beeeabf9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_all_data():\n",
    "    call_get_all_players() # Gather summonerIds\n",
    "    call_puuids() # Gather puuids\n",
    "    call_match_list() # Gather list of matchIds\n",
    "    call_store_matches() # Gather match statistics\n",
    "    call_gather_stats() # Generate percentiles and stds based on champions and players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8378c16-cc74-4205-89b6-7ea0d0715ab1",
   "metadata": {},
   "source": [
    "These next 2 functions are called in the dataloader. find_stats takes all of the values stored in player_stas and champ_stats that are related to the specific match id given, so it uses the related players and champions. find_labels instead takes the related values from player_matches and team_matches, the true values of the match played. Both functions are essentially repeated calls to their respective two tables in match_database. The major difference between the two is how they are stored in the database. Where each value that find_labels looks at is a single float, each value find_stats looks at is a list of 4 values. I wanted each input to be one large tensor of values, so I had to extract the tensor, and use .view(-1) on it to seamlessly add it to the tensor. Also due to the NaN issue from gather_stats, I added the conditional to check if the std failed, and replace with a 0 float instead.  \n",
    "  \n",
    "The next step is some quick data cleaning, as after running call_all_data(), not everything is perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9788887-c731-472f-b5cc-f82b11a1397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conn = sqlite3.connect(match_file_path)\n",
    "match_cursor = match_conn.cursor()\n",
    "\n",
    "# Some matches only get 8 entries stored, with no team position\n",
    "match_cursor.execute(\"DELETE FROM player_matches WHERE teamPosition = '' OR teamPosition IS NULL\")\n",
    "\n",
    "# Some teams end up with multiple instances of the same team, although this may have been because I ran store_matches multiple times and never removed the inputs\n",
    "match_cursor.execute(\"\"\"\n",
    "    DELETE FROM team_matches\n",
    "    WHERE rowid NOT IN (\n",
    "        SELECT MIN(rowid)\n",
    "        FROM team_matches\n",
    "        GROUP BY win, matchId\n",
    "    )\n",
    "\"\"\")\n",
    "match_conn.commit()\n",
    "\n",
    "# Some matches simply lose a player, and I can't run a model down a player\n",
    "match_cursor.execute(\"\"\"DELETE FROM player_matches\n",
    "WHERE matchId IN (\n",
    "    SELECT matchId\n",
    "    FROM player_matches\n",
    "    GROUP BY matchId\n",
    "    HAVING COUNT(*) = 9\n",
    ");\"\"\")\n",
    "match_conn.commit()\n",
    "\n",
    "# Some matches simply lose a team as well, and I can't run a model down a team\n",
    "match_cursor.execute(\"\"\"DELETE FROM player_matches\n",
    "WHERE matchId IN (\n",
    "    SELECT matchId\n",
    "    FROM team_matches\n",
    "    GROUP BY matchId\n",
    "    HAVING COUNT(*) = 1\n",
    ");\"\"\")\n",
    "match_conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d2ac0-b33f-44f8-8187-5128174004fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8df6d9c-0a9c-4a2b-ac3c-3cf58757c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stats(match_id):\n",
    "    # Given a match id, return all of the aggregated performanced related to that match (data for model)\n",
    "    input_list = []\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    \n",
    "    # Gather the the info from the database\n",
    "    match_participants = match_cursor.execute(\"SELECT championName, puuid, teamPosition FROM player_matches WHERE matchId = ?\", (str(match_id),)).fetchall()\n",
    "\n",
    "    for participant in match_participants:\n",
    "        # Both of these calls only return one row, so the next for loops will automatically index into the only row gotten from the call\n",
    "        match_champ_stats = match_cursor.execute(\"SELECT * FROM champ_stats WHERE championName = ? AND teamPosition = ?\", (participant[0], participant[2],)).fetchall()\n",
    "        match_player_stats = match_cursor.execute(\"SELECT * FROM player_stats WHERE puuid = ? AND teamPosition = ?\", (participant[1], participant[2],)).fetchall()\n",
    "        \n",
    "        # Skipping the first 2 columns since they are identifying columns\n",
    "        for stat in match_champ_stats[0][2:]:\n",
    "            stat_list = json.loads(stat)\n",
    "            stat_tensor = torch.tensor(stat_list)\n",
    "            # Some of the standard deviation values were marked as NaN, so this conditional handles replacement\n",
    "            if torch.isnan(stat_tensor[0][-1]):\n",
    "                stat_tensor[0][-1] = 0.0\n",
    "            input_list.append(stat_tensor.view(-1))\n",
    "        \n",
    "        for stat in match_player_stats[0][2:]:\n",
    "            stat_list = json.loads(stat)\n",
    "            stat_tensor = torch.tensor(stat_list)\n",
    "            if torch.isnan(stat_tensor[0][-1]):\n",
    "                stat_tensor[0][-1] = 0.0\n",
    "            input_list.append(stat_tensor.view(-1))\n",
    "            \n",
    "    match_conn.close()\n",
    "    # Concatenates the input list of tensors into one row\n",
    "    return torch.cat(input_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc35971-ff61-4c28-a337-bf13a4dd32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(match_id):\n",
    "    # Given a match id, return all of the statistics related to that match (labels for model)\n",
    "    label_list = []\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    \n",
    "    # Gather the the info from the database\n",
    "    player_match_vals = match_cursor.execute(\"SELECT * FROM player_matches WHERE matchId = ?\", (str(match_id),)).fetchall()\n",
    "    team_match_vals = match_cursor.execute(\"SELECT * FROM team_matches WHERE matchId = ?\", (str(match_id),)).fetchall()\n",
    "    match_conn.close()\n",
    "    \n",
    "    #loop through the player rows, and then the team rows, concatenating all into one large tensor\n",
    "    for player in player_match_vals:\n",
    "        for column in player[4:]:\n",
    "            label_list.append(float(column))\n",
    "\n",
    "    for team in team_match_vals:\n",
    "        for column in team[1:]:\n",
    "            label_list.append(float(column))\n",
    "\n",
    "    # Creates a tensor out of the label_list\n",
    "    return torch.tensor(label_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a528296-b38c-4cbb-9428-8d14edda9193",
   "metadata": {},
   "source": [
    "This dataset is fairly standard compared to the ones we've seen in class, it has inputs and labels as attribrutes of the class, and a get item method that gathers them when called. The main difference is that as an argument the class knows if it is a training or test module, and creates the dataset as such. If I want to get different training/test datasets, I can always modify the random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f915f70-85ec-47d5-8b07-61f55b5ab689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameDataset(Dataset):\n",
    "    def __init__(self, incorrect_match_ids, train):\n",
    "        # The matchIds come in as strings, with parentheses attached, so this for loop removes them making suitable for sqlite queries\n",
    "        match_ids = [match_id[0] for match_id in incorrect_match_ids]\n",
    "        self.stats = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Creates a split of the matchIds to usefor the data\n",
    "        train_match_ids, test_match_ids = train_test_split(match_ids, test_size=0.1, random_state=42)\n",
    "\n",
    "        # This conditional selects which matchids will be used for the dataset\n",
    "        if train is True:\n",
    "            self.match_ids = train_match_ids\n",
    "        else:\n",
    "            self.match_ids = test_match_ids \n",
    "            \n",
    "        for match_id in self.match_ids:\n",
    "            stats = find_stats(match_id)\n",
    "            labels = find_labels(match_id)\n",
    "            self.stats.append(stats)\n",
    "            self.labels.append(labels)\n",
    "            \n",
    "            # Tracks progress for creating a dataset object\n",
    "            if len(self.stats) % 1000 == 0:\n",
    "                print(len(self.stats))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stats)\n",
    "\n",
    "    def __getitem__(self, ids):\n",
    "        return self.stats[ids], self.labels[ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader():\n",
    "    match_conn = sqlite3.connect(match_file_path)\n",
    "    match_cursor = match_conn.cursor()\n",
    "    matches = match_cursor.execute(\"SELECT DISTINCT matchId FROM player_matches\").fetchall()\n",
    "    match_conn.close()\n",
    "    print(\"beginning attempt\")\n",
    "    train_dataset = GameDataset(matches, train=True)\n",
    "    print(\"almost there...\")\n",
    "    test_dataset = GameDataset(matches, train=False)\n",
    "    train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    print(\"finished!\")\n",
    "    return train_dl, test_dl\n",
    "\n",
    "train_dl, test_dl = create_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aece1a2-2ec9-40ff-b810-b8003d96f4ee",
   "metadata": {},
   "source": [
    "I decided to go with use of fully connected layers, with some splashes of normalization and regularization. I normalized each layer before running it through a fully connected layer. I believed fully connected layers would prove incredibly valuable for this problem. For example, if one top laner is playing a very strong champion, and also performs much better at the role than his opponent, then that would affect the prediction of how many kills and deaths both top laners would have, but it should have little effect on the kills and deaths of the bottom laner, since they are across the map. By doing this, the model could supposedly see which values have the biggest impact for each player. Unfortunately the model does not seem very effective, and isn't able to make sense of any of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13616b14-4c94-4601-8528-367e18c4c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.BatchNorm1d(1760)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(1550)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(1400)\n",
    "        self.norm4 = torch.nn.BatchNorm1d(1250)\n",
    "        self.norm5 = torch.nn.BatchNorm1d(1100)\n",
    "        self.norm6 = torch.nn.BatchNorm1d(900)\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)  \n",
    "        self.dropout2 = torch.nn.Dropout(0.2)  \n",
    "        self.fc1 = torch.nn.Linear(1760, 1550)\n",
    "        self.fc2 = torch.nn.Linear(1550, 1400)\n",
    "        self.fc3 = torch.nn.Linear(1400, 1250)\n",
    "        self.fc4 = torch.nn.Linear(1250, 1100)\n",
    "        self.fc5 = torch.nn.Linear(1100, 900)\n",
    "        self.fc6 = torch.nn.Linear(900, 600)\n",
    "        self.fc7 = torch.nn.Linear(600, 400)\n",
    "        self.fc8 = torch.nn.Linear(400, 266)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm3(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.norm4(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.norm5(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.norm6(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        return x\n",
    "\n",
    "model = GameModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28860f-0bb8-4488-8c31-4e92b429062e",
   "metadata": {},
   "source": [
    "Standard train_model function, I used MSELoss since the data was not catagorical, and adam optimizer since it seems like its the best. Most of this code is taken directly by my wonderful professor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1db31-2f74-4ce8-8d39-591a9b1a16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, learning_rate, dataloader):\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 90:    # print every 100 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 6400:.3f}')\n",
    "                    running_loss = 0.0\n",
    "    print(f\"Finished Training.  Elapsed time: {time.time()-start_time:0.2f}\")\n",
    "    print(f\"Finished Training.  Elapsed time: {time.time()-start_time:0.2f}\")\n",
    "\n",
    "train_model(model, 70, 0.001, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288b99d-c565-4187-81d9-2b2bd2081ae3",
   "metadata": {},
   "source": [
    "I wanted to find out by what percent are my predictions are off by, so I can know how to modify my model. This function provides a percent for each batch in the testing batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdb32a94-3058-48a8-8e98-5cb2ac468cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4.3720e+13),\n",
       " tensor(3.9440e+13),\n",
       " tensor(3.6535e+13),\n",
       " tensor(4.5583e+13),\n",
       " tensor(4.0176e+13),\n",
       " tensor(4.4834e+13),\n",
       " tensor(4.1637e+13),\n",
       " tensor(4.7029e+13),\n",
       " tensor(4.2871e+13),\n",
       " tensor(3.8384e+13),\n",
       " tensor(4.2960e+13),\n",
       " tensor(3.9691e+13),\n",
       " tensor(4.0508e+13),\n",
       " tensor(3.6381e+13),\n",
       " tensor(3.9320e+13),\n",
       " tensor(3.7066e+13),\n",
       " tensor(4.0170e+13),\n",
       " tensor(3.9836e+13),\n",
       " tensor(3.6931e+13),\n",
       " tensor(1.4939e+13)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy_percent(given_model, dataloader):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = given_model(inputs) # Model makes predictions\n",
    "        abs_diff = torch.abs(outputs - labels) # Find absolute value difference in the predictions\n",
    "        percentages = (abs_diff / (labels + 0.0000000001)) * 100 # Calculate the percent, while avoiding infinite values\n",
    "        all_predictions.append(torch.mean(percentages)) # Add the average percent difference to return\n",
    "        \n",
    "    return all_predictions\n",
    "\n",
    "accuracy_percent(model, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df79ba1-c999-480f-87a2-e6dfad637bae",
   "metadata": {},
   "source": [
    "# Results  \n",
    "  \n",
    "Unfortunately I could not get the model to provide anything useful. The predicted outputs have massive losses compared to the real outcomes of the games. The loss is quite high, and even eyeballing the results shows that the predictions are rarely near what they should be. train_model usually outputs losses during training upwards of 1 million. I was a little rushed in analyzing the data, and it is possible that the model isn't quite as innacurate as I think it is now, its difficult to say since so many of the values have different scales (damage to champions is measured in tens of thousands, kills is usually single digits)\n",
    "  \n",
    "It's difficult to pinpoint exactly where it went wrong, maybe I didn't have enough features, matches, the task was way beyond me in the first place or I messed up execution of the model. I believe there is some truth to all four possibilities, thought it would be difficult to assign a percentage. I ended up only using 22 features to try to make all these predictions, and maybe that isnt enough information to go off of. It's likely that even the whole api doesn't have enough info to make better predictions. Ideally we would take in player positioning, how much damage dealt at different points in the game, or other even more match specific information. Although the IQR + STD method made the model \"work\" it likely is too inspecific to get meaningful data out of. As well after data cleaning, I only ended up with about 12,000 games, which considering the amount of players and champions, isn't really a lot to make good predictions. Not to mention the data was taken across multiple balance patches and ranks, further diluting the quality. Perhaps if I took another stab at it, the project could turn out better, or perhaps theres inherently not enough data to account for the randomness of human action in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8fd86-d56b-42bc-94b5-70cbcd2a4ee2",
   "metadata": {},
   "source": [
    "# Conclusion  \n",
    "  \n",
    "In this project I gathered information on a great number of league games and tried to see how much of the game could be predicted by the players and champions within it. It required learning about using an API, and slowly step by step gathering the info that allowed me to get the statistics I wanted to train on. Once I had them, I could begin setting up the model to train on the data gathered and see how effective it really was. I used an end to end model which was a simple way to process all the data, but a GRU could also possibly provide effective results.  \n",
    "  \n",
    "I learned a lot over the course of this project, first being about using an api. How to look through documentation, make requests, and then also storing that information in sqlite was definitely very valuable. This project also showed a lot of value in planning, as had I planned better it would have gone a little smoother. I'm always tempted to rush into projects to get started instantly, but this taught me the value of looking at the bigger picture and planning my inputs and outputs (kind of like planning a cpu, sort of). I lost a lot of time attempting long runs of code without being 100% certain that it will work, which I will try not to repeat in future projects. By being greedy and not testing properly before running code, I ended up wasting far more time than if I had been careful in the first place. As well, using the time module to help give a sense of how long something takes, and if that is correct. It will greatly help any future project I do, even if its not related to deep learning. I learned a lot about shaping up the various inputs and outputs of the databases, datasets, and dataloaders. Towards the end I also researched more possible deep learning models that may have given a better outcome than my own.  \n",
    "  \n",
    "There are quite a few ways the project could be continued from here. I unfortunately didn't end up training the model as much as I would have liked, so I'm sure it could've been more accurate. Beyond that a GRU model could be an interesting way to go, as well as selecting more features than I did to make predictions.  I tried to stick with features I knew would be valuable, to minimze redundancy. As well, a CNN could also possibly have had some success, since the way I adapted the input data could be read by one and possibly have found patterns. Making the model more conventionally useful by adding a function that accepts the list of players and champions from a new game, and runs the inference very quickly, for when I am about to play a game, essentially making the whole project quick and easy to use. This of course would have to come after finding a working model for this problem. Lastly, pivoting to finding just the likelihood of one team winning, as opposed to every possible statistic could make for a more accurate project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
